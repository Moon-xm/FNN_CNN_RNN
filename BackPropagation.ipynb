{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 上课示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 3070.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before training： 4 4.0\n",
      "after 10 epoch, loss:0.03185431286692619\n",
      "after 20 epoch, loss:7.580435340059921e-05\n",
      "after 30 epoch, loss:1.805076408345485e-07\n",
      "after 40 epoch, loss:4.4019543565809727e-10\n",
      "after 50 epoch, loss:2.0463630789890885e-12\n",
      "after 60 epoch, loss:9.094947017729282e-13\n",
      "after 70 epoch, loss:9.094947017729282e-13\n",
      "after 80 epoch, loss:9.094947017729282e-13\n",
      "after 90 epoch, loss:9.094947017729282e-13\n",
      "after 100 epoch, loss:9.094947017729282e-13\n",
      "after training： 4 7.999998569488525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "x_data = [1.0, 2.0, 3.0]\n",
    "y_data = [2.0, 4.0, 6.0]\n",
    "\n",
    "w = torch.Tensor([1.0])\n",
    "w.requires_grad = True\n",
    "\n",
    "def forward(x):\n",
    "    y_pred = w*x \n",
    "    return y_pred\n",
    "    \n",
    "def loss(x, y):\n",
    "    return (forward(x) - y)**2\n",
    "print('before training：', 4, forward(4).item())  # 训练前预测x = 4 对应的 y  此时w是初始化的w = 1\n",
    "\n",
    "epoch = 100\n",
    "learning_rate = 0.01\n",
    "for i in tqdm(range(epoch)):\n",
    "    for x, y in zip(x_data, y_data):\n",
    "        loss_val = loss(x, y)\n",
    "        loss_val.backward()  # 自动计算所有梯度，此时计算图被释放（动态）\n",
    "        w.data = w.data - learning_rate*w.grad.data  # 取data进行计算就不会计算计算图，虽然后面显示不加data表示的内容也一样\n",
    "#         print('\\tw.grad.item() =', w.grad.item(), 'w.grad=',w.grad,'w.grad.data=',w.grad.data)  # item（）表示变成标量 ,w.grad是一个Tensor\n",
    "        w.grad.data.zero_()  # 不要忘记将w的梯度置0 此处也是对data进行操作\n",
    "    if (i+1) %10 == 0:\n",
    "#         print('loss_val=',loss_val,'loss_val.data=',loss_val.data,'loss_val.item()=',loss_val.item())  # item()表示输出标量形式\n",
    "        print('after {} epoch, loss:{}'.format(i+1, loss_val.item()))\n",
    "\n",
    "print('after training：', 4, forward(4).item())  # 训练后预测x = 4 对应的y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 课后习题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 2113.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training:x = 4, y_pred = 21.0\n",
      "w1 =  1.0 ,w2 =  1.0 ,b =  1.0\n",
      "\tafter 10 epoch, loss = 0.7234453558921814\n",
      "\tafter 20 epoch, loss = 0.037623144686222076\n",
      "\tafter 30 epoch, loss = 0.01856783591210842\n",
      "\tafter 40 epoch, loss = 0.01676689274609089\n",
      "\tafter 50 epoch, loss = 0.01615307480096817\n",
      "\tafter 60 epoch, loss = 0.015657441690564156\n",
      "\tafter 70 epoch, loss = 0.015192092396318913\n",
      "\tafter 80 epoch, loss = 0.014748352579772472\n",
      "\tafter 90 epoch, loss = 0.014324654825031757\n",
      "\tafter 100 epoch, loss = 0.01391984336078167\n",
      "After training:x = 4, y_pred = 9.07545280456543\n",
      "w1 =  0.3140466809272766 ,w2 =  0.780616283416748 ,b =  0.9282406568527222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 准备数据\n",
    "x_data = [1.0, 2.0, 3.0]\n",
    "y_data = [2.0, 4.0, 6.0]\n",
    "\n",
    "w1 = torch.Tensor([1.0])\n",
    "w1.requires_grad = True\n",
    "w2 = torch.Tensor([1.0])\n",
    "w2.requires_grad = True\n",
    "b = torch.Tensor([1.0])\n",
    "b.requires_grad = True\n",
    "\n",
    "\n",
    "def forward(x):\n",
    "    y_pred = w1*x**2 + w2*x + b\n",
    "    return y_pred\n",
    "\n",
    "def loss(x, y):\n",
    "    return (forward(x) - y)**2\n",
    "\n",
    "epoch = 100\n",
    "learning_rate = 0.001\n",
    "print('Before training:x = {}, y_pred = {}'.format(4, forward(4).item()))\n",
    "print('w1 = ',w1.data.item(), ',w2 = ',w2.data.item(),',b = ',b.data.item())\n",
    "for i in tqdm(range(epoch)):\n",
    "    for x,y in zip(x_data, y_data):\n",
    "        l = loss(x,y)\n",
    "        l.backward()\n",
    "        w1.data = w1.data -  learning_rate * w1.grad.data\n",
    "        w2.data -= learning_rate * w2.grad.data\n",
    "        b.data -= learning_rate * b.grad.data\n",
    "        w1.grad.data.zero_()\n",
    "        w2.grad.data.zero_()\n",
    "        b.grad.data.zero_()\n",
    "    if (i+1) % 10 == 0:\n",
    "        print('\\tafter {} epoch, loss = {}'.format(i+1, l.item()))\n",
    "\n",
    "print('After training:x = {}, y_pred = {}'.format(4, forward(4).item()))\n",
    "print('w1 = ',w1.data.item(), ',w2 = ',w2.data.item(),',b = ',b.data.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
